This directory explores an unsupervised approach to training a neural network (CNN in this case) designed to perform a type of "soft clustering". I try to maximize within-batch variance of network outputs but minimize entropy for each (softmax) output, to see what would happen. Will the network naturally assign a label to each input that says something meaningful about the input (i.e. is this a form of clustering where k = the output dimensionality of the network)?

From testing, learning seemed highly unstable for a wide range of learning rates, and converged upon undesired solutions (for example, outputs tended to be confident for one of two classes of the total 10). It's also unclear if the custom loss function provides a specific-enough optimization goal that would mimic clustering, even if a good minimum were found.
